{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad64dee",
   "metadata": {},
   "source": [
    "# Step 0: Latent Dirichlet Allocation\n",
    "I'm going to start by trying [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) for classification. I'm not going to go into how it does what it does, I'm just goint to use it. LDA classifies text in a document to a particular topic.\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "I'm going to start with a subset of the RSS datafeeds from NYTimes and WSJ. For now, I'm going to add some code to pull this data out of our database rather than putting the data in a CSV file.  This will make filtering the data much easier than having to produce a new csv file every time we want to change something.\n",
    "\n",
    "Notes:\n",
    "1. I'm using sqlalchemy and pymysql rather than Oracle's sql connector because I want to put the data into a padas frame. They are infinitely more suited to analysis than python's default list.\n",
    "\n",
    "**Dataset Inputs**  \n",
    "MaxRows = maximum number of rows to load from the database  \n",
    "DtStart = look for articles published on or after this date  \n",
    "DtStop  = look for articles published before this date  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13a0a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Title  index\n",
      "0    When Is It Too Late to Buy the Hottest Gifts O...      0\n",
      "1              Robots vs. Fatbergs in America's Sewers      1\n",
      "2    Bitcoin's Creator Could Be Unmasked at Florida...      2\n",
      "3     Toxic Positivity Is Very Real, and Very Annoying      3\n",
      "4    Daylight-Saving Time Ends Sunday. Four Ways to...      4\n",
      "..                                                 ...    ...\n",
      "452          Vienna Reels From a Rare Terrorist Attack    452\n",
      "453          Coronavirus Briefing: What Happened Today    453\n",
      "454  She Was Losing Fistfuls of Hair. What Was Caus...    454\n",
      "455  Think You Have ‘Normal’ Blood Pressure? Think ...    455\n",
      "456  Navigating My Son’s A.D.H.D. Made Me Realize I...    456\n",
      "\n",
      "[457 rows x 2 columns]\n",
      "                                                 Title  index\n",
      "0    when is it too late to buy the hottest gifts o...      0\n",
      "1              robots vs. fatbergs in america's sewers      1\n",
      "2    bitcoin's creator could be unmasked at florida...      2\n",
      "3     toxic positivity is very real, and very annoying      3\n",
      "4    daylight-saving time ends sunday. four ways to...      4\n",
      "..                                                 ...    ...\n",
      "452          vienna reels from a rare terrorist attack    452\n",
      "453          coronavirus briefing: what happened today    453\n",
      "454  she was losing fistfuls of hair. what was caus...    454\n",
      "455  think you have ‘normal’ blood pressure? think ...    455\n",
      "456  navigating my son’s a.d.h.d. made me realize i...    456\n",
      "\n",
      "[457 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "dburl     = 'mysql+pymysql://ec2-user@localhost:3306'\n",
    "sqlEngine = create_engine(dburl)\n",
    "cnx       = sqlEngine.connect()\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "#  Set query variables and get the data\n",
    "#-------------------------------------------------------------\n",
    "MaxItems  = 10000\n",
    "DtStart   = \"2020-09-01\"\n",
    "DtStop    = \"2021-11-15\"\n",
    "q         = 'SELECT Title FROM plato.Item WHERE \"{}\" <= PubDt AND PubDt < \"{}\" LIMIT {};'.format(DtStart,DtStop,MaxItems)\n",
    "documents = pd.read_sql(q, cnx)\n",
    "documents['index'] = documents.index\n",
    "print(documents)\n",
    "documents['Title'] = documents['Title'].apply(lambda s: s.lower())\n",
    "print(documents)\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78f987",
   "metadata": {},
   "source": [
    " # Step 2: Data Processing\n",
    " * **Tokenize**: split the text into sentences, the sentences into words, lower case, remove punctuation\n",
    " * Remove 1- and 2-character words\n",
    " * Remove **stopwords** (the, ...)\n",
    " * **Lemmatize**: change all verb tenses to present tense\n",
    " * **Stem**: reduce words to their root form\n",
    " \n",
    "**Notes**  \n",
    "The simple example cases for <code>nltk.download('wordnet')</code> I've seen on the net do not have any of the ssl stuff.  I tried it that way first, and it resulted in an SSL error. The ssl code shown below comes from a [Stackoverflow article](https://stackoverflow.com/questions/38916452/nltk-download-ssl-certificate-verify-failed), apparently others hit this problem as well.\n",
    "\n",
    "**Notes**  \n",
    "<code>gensim</code> is an open-source, production-ready, machine learning library for unsupervised topic modeling. Seems like it will be perfect for what we have in mind.  \n",
    "<code>nltk</code> is a Natural Language Tool Kit, it is a standard for processing text and seems to be used a lot in machine learning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cced6c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/sman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(10000000)\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab7f8d",
   "metadata": {},
   "source": [
    "## Lemmatize Examples\n",
    "Just to get the idea, look what happens when we lemmatize \"went\", \"gone\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f54d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went -> go\n",
      "gone -> go\n",
      "going -> go\n"
     ]
    }
   ],
   "source": [
    "for w in [\"went\", \"gone\", \"going\"]:\n",
    "    print( \"{} -> {}\".format(w,WordNetLemmatizer().lemmatize(w, pos='v')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a88211",
   "metadata": {},
   "source": [
    "## Stemmer Examples\n",
    "Here are some examples that show how the stemmer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5678b0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Word</th>\n",
       "      <th>Output Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sensation</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tradition</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>colony</td>\n",
       "      <td>coloni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colon</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Input Word Output Word\n",
       "0      caresses      caress\n",
       "1         flies         fli\n",
       "2          dies         die\n",
       "3         mules        mule\n",
       "4        denied        deni\n",
       "5          died         die\n",
       "6        agreed        agre\n",
       "7       humbled       humbl\n",
       "8       meeting        meet\n",
       "9   itemization        item\n",
       "10  sensational      sensat\n",
       "11    sensation      sensat\n",
       "12  traditional      tradit\n",
       "13    tradition      tradit\n",
       "14    reference       refer\n",
       "15       colony      coloni\n",
       "16    colonizer       colon\n",
       "17        colon       colon"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "wordsIn = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died', 'agreed', \n",
    "           'humbled', 'meeting', 'itemization', 'sensational', 'sensation',\n",
    "           'traditional', 'tradition', 'reference', 'colony', 'colonizer', 'colon']\n",
    "wordsOut = [stemmer.stem(plural) for plural in wordsIn]\n",
    "pd.DataFrame(data={'Input Word':wordsIn, 'Output Word':wordsOut})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d7feb",
   "metadata": {},
   "source": [
    "## Process the Article Titles\n",
    "The function <code>lemstem</code> simply applies the lemmatizer and stemmer on the supplied text. The function <code>preprocess</code> tokenizes the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bce03f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemstem(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >= 3:\n",
    "            result.append(lemstem(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c4e128",
   "metadata": {},
   "source": [
    "To validate this code, let's just grab a random title from the document and run it through <code>preprocess</code> and review what comes back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7cb95758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document:\n",
      "['a', 'dispatch', 'from', 'an', 'endangered', 'bird’s', '‘garden', 'of', 'eden’']\n",
      "\n",
      "\n",
      "preprocessed document:\n",
      "['dispatch', 'endang', 'bird', 'garden', 'eden']\n"
     ]
    }
   ],
   "source": [
    "docno = 357  # pick one at random\n",
    "sample = documents[documents['index'] == docno].values[0][0]\n",
    "print(\"original document:\")\n",
    "words = []\n",
    "for w in sample.split(' '):\n",
    "    words.append(w)\n",
    "print(words)\n",
    "print(\"\\n\\npreprocessed document:\")\n",
    "print(preprocess(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31026b",
   "metadata": {},
   "source": [
    "Now let's process everything. This step may take a while if the dataset gets large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "899147a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      [late, buy, hottest, gift, onlin]\n",
      "1                       [robot, fatberg, america, sewer]\n",
      "2             [bitcoin, creator, unmask, florida, trial]\n",
      "3                            [toxic, posit, real, annoy]\n",
      "4      [daylight, save, time, end, sunday, way, win, ...\n",
      "                             ...                        \n",
      "452              [vienna, reel, rare, terrorist, attack]\n",
      "453                  [coronavirus, brief, happen, today]\n",
      "454                             [lose, fist, hair, caus]\n",
      "455               [think, normal, blood, pressur, think]\n",
      "456                                 [navig, son, realiz]\n",
      "Name: Title, Length: 457, dtype: object\n"
     ]
    }
   ],
   "source": [
    "processed_docs = documents['Title'].map(preprocess)\n",
    "print(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e51aed",
   "metadata": {},
   "source": [
    " ## Step 3.1  \"Bag of Words\" on the dataset\n",
    " Based on the frequency of the words, extract topics. That is, create a dictionary form <code>processedDocs</code> containing the number of times the words appear in the \"training set\" (the article Titles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "653cb9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916eb311",
   "metadata": {},
   "source": [
    "Let's print out the top 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e755268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 buy\n",
      "1 gift\n",
      "2 hottest\n",
      "3 late\n",
      "4 onlin\n",
      "5 america\n",
      "6 fatberg\n",
      "7 robot\n",
      "8 sewer\n",
      "9 bitcoin\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k,v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    i += 1\n",
    "    if i > 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff71131",
   "metadata": {},
   "source": [
    "To make the ai training faster, we can filter out the extremes. Very rare words are probably not significant. The talks I've listened to say that words appearing in more than 50% of the documents are probably not significant (may be true but this has not been settled for me yet, I will probably change the params around a bit and try to understand this better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d27f302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=5, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff677044",
   "metadata": {},
   "source": [
    "Now we want to use the <code>doc2bow</code> function to create a bag-of-words... a list of 2-tuples (token_id, token_count).  <code>doc2bow</code> requires that each word is tokenized and normalized (either unicode or utf8), so we'll use <code>processed_docs</code> from above. Essentially, we end up with a dictionary that reports how many words and how many times those words appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "21e6ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6364e5e",
   "metadata": {},
   "source": [
    "Do a quick check on one of the rows... we'll use index <code>docno</code>.  For each tuple, the first number is the index of the word, the second number is the count for the word in the title.  For most titles, the count will be 1 for every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "630fc19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "bow_docno = bow_corpus[docno]\n",
    "print(bow_docno)\n",
    "for i in range(len(bow_docno)):\n",
    "    print('Word {} (\"{}\") count: {}'.format(bow_docno[i][0],\n",
    "                                            dictionary[bow_docno[i][0]],\n",
    "                                            bow_docno[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75985b27",
   "metadata": {},
   "source": [
    "## Step 3.2 TF-IDF the Document Set\n",
    "While performing TF-IDF on the corpus is not necessary for LDA implementation using the gensim model, it is recommended. TF-IDF expects a bag-of-words (integer values) training corpus dor initialization. During transformation, it will take a vector and return another vector of the same dimensions.\n",
    "\n",
    "_Note: The author of gensim dictates the standard procedure for LDA to be using the Bag of Words model._\n",
    "\n",
    "TF-IDF stands for \"Term Frequency, Inverse Document Frequency\"\n",
    "\n",
    "* It is a way to score the importance of words in a document based on how frequently they appear across multiple documents.\n",
    "* If a word appears frequently in a document, then it is important and will be given a high score. But if a word appears  in many documents, it is not a unique identifier and will be given a lower score.\n",
    "* So, common words like \"the\" and \"for\" will be scaled down. Words that appear frequently in a single document will be scaled up.\n",
    "\n",
    "In other words:\n",
    "\n",
    "* TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)\n",
    "* IDF(w) = log_e(Total number of documents / Number of documents with term w in it).\n",
    "\n",
    "Example\n",
    "* Consider a document containing 100 words and the word \"tiger\" appears 3 times.\n",
    "* The term frequency (TF) for \"tiger\" is:\n",
    "  * TF = (3/100) = 0.03\n",
    "* Now, assume we have 10 million documents and the word \"tiger\" appears in 1000 of them. Then, the indverse document frequency (IDF) is:\n",
    "  * IDF = log(10,000,000 / 1,000) = 4\n",
    "* Thus, the TF-IDF weight is the product of these quantities:\n",
    "  * TFIDF = 0.03 * 4 = 0.12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c316226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=457, num_nnz=546)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9235eb",
   "metadata": {},
   "source": [
    "Apply tranformation to the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b2c8482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1.0)]\n",
      "[(1, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "print(corpus_tfidf[1])\n",
    "from pprint import pprint\n",
    "pprint(corpus_tfidf[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b97893",
   "metadata": {},
   "source": [
    "## Step 4:  Running LDA using Bag of Words\n",
    "The model is finally ready to do the unsupervised learning. We choose the number of topics we want it to find.\n",
    "\n",
    "Some of the parameters:\n",
    "\n",
    "* **num_topics** is the number of requested latent topics to be extracted from the training corpus\n",
    "* **id2word** is a mapping from word ids (integers) to words (strings).  It is used to determin the vocabulary size, as well as for debugging and topic printing.\n",
    "* **alpha** and **eta** are hyperparameters that affect sparsity of the document-topic (theta) and topic-word (lambda) distributions. We will let these be the default values for now (default value is 1/num_topics).\n",
    "  * **Alpha** is the per-document topic distribution.\n",
    "    * High alpha: Every document has a mixture of all topics (documents appear similar to each other).\n",
    "    * Low alpha: Every document has a mixture of very few topics\n",
    "  * **Eta** is the per-topic word distribution\n",
    "    * High eta: Each topic has a mixture of most words (the docs appear similar to each other).\n",
    "    * Low eta: Ech topic a mixture of few words.\n",
    "  * **passes**: the number of training passes through the corpus.  For example if the training corpus has 50,000 documents, and the chunksize is 10,000, and passes is 2, then online training is done in 10 updates:\n",
    "    * update  1: docs      0 - 9,999\n",
    "    * update  2: docs 10,000 - 19,999\n",
    "    * ...\n",
    "    * update  9: docs 30,000 - 39,999\n",
    "    * update 10: docs 40,000 - 49,999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5494a5",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "First, I'm going to list the fallback code, which is based on the LDA mono-core. We'll only use this if LdaMulticore throws an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a077a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model = gensim.models.LdaModel(bow_corpus, num_topics = 10, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658825a",
   "metadata": {},
   "source": [
    "We'll train the model using LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b26e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: 0.130*\"make\" + 0.096*\"american\" + 0.060*\"tri\" + 0.060*\"game\" + 0.049*\"best\" + 0.049*\"old\" + 0.043*\"power\" + 0.037*\"rule\" + 0.037*\"year\" + 0.037*\"electr\"\n",
      "\n",
      "\n",
      "Topic: 1\n",
      "Words: 0.117*\"cup\" + 0.112*\"team\" + 0.100*\"lead\" + 0.090*\"ryder\" + 0.056*\"final\" + 0.041*\"week\" + 0.031*\"busi\" + 0.031*\"best\" + 0.027*\"world\" + 0.026*\"win\"\n",
      "\n",
      "\n",
      "Topic: 2\n",
      "Words: 0.135*\"covid\" + 0.118*\"democrat\" + 0.097*\"climat\" + 0.070*\"plan\" + 0.057*\"virus\" + 0.043*\"time\" + 0.043*\"report\" + 0.036*\"world\" + 0.022*\"take\" + 0.022*\"work\"\n",
      "\n",
      "\n",
      "Topic: 3\n",
      "Words: 0.152*\"new\" + 0.131*\"review\" + 0.071*\"long\" + 0.068*\"big\" + 0.067*\"appl\" + 0.066*\"season\" + 0.054*\"electr\" + 0.049*\"vaccin\" + 0.041*\"say\" + 0.023*\"week\"\n",
      "\n",
      "\n",
      "Topic: 4\n",
      "Words: 0.083*\"war\" + 0.078*\"chang\" + 0.072*\"kill\" + 0.072*\"star\" + 0.066*\"space\" + 0.065*\"covid\" + 0.058*\"rule\" + 0.058*\"colleg\" + 0.049*\"report\" + 0.031*\"offer\"\n",
      "\n",
      "\n",
      "Topic: 5\n",
      "Words: 0.163*\"new\" + 0.068*\"inflat\" + 0.059*\"fight\" + 0.050*\"retir\" + 0.050*\"onlin\" + 0.046*\"biden\" + 0.046*\"life\" + 0.041*\"american\" + 0.037*\"work\" + 0.037*\"insid\"\n",
      "\n",
      "\n",
      "Topic: 6\n",
      "Words: 0.186*\"home\" + 0.095*\"world\" + 0.079*\"lose\" + 0.073*\"nasa\" + 0.073*\"face\" + 0.067*\"arrest\" + 0.037*\"space\" + 0.025*\"die\" + 0.019*\"best\" + 0.019*\"plan\"\n",
      "\n",
      "\n",
      "Topic: 7\n",
      "Words: 0.150*\"climat\" + 0.095*\"talk\" + 0.084*\"money\" + 0.078*\"high\" + 0.073*\"glasgow\" + 0.057*\"beat\" + 0.046*\"year\" + 0.046*\"return\" + 0.035*\"old\" + 0.029*\"chang\"\n",
      "\n",
      "\n",
      "Topic: 8\n",
      "Words: 0.092*\"start\" + 0.086*\"charg\" + 0.079*\"new\" + 0.072*\"time\" + 0.072*\"face\" + 0.072*\"golf\" + 0.072*\"basketbal\" + 0.044*\"health\" + 0.040*\"colleg\" + 0.037*\"busi\"\n",
      "\n",
      "\n",
      "Topic: 9\n",
      "Words: 0.096*\"brief\" + 0.088*\"playoff\" + 0.084*\"die\" + 0.080*\"worker\" + 0.072*\"coronavirus\" + 0.072*\"colleg\" + 0.063*\"footbal\" + 0.055*\"america\" + 0.047*\"art\" + 0.043*\"elect\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word = dictionary, passes = 2, workers = 2)\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# For each topic, show the words occurring in that topic and its relative weight:\n",
    "#----------------------------------------------------------------------------------\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {}\\nWords: {}\\n\\n\".format(idx,topic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d871e",
   "metadata": {},
   "source": [
    "Check which topic our test document (at index <code>docno</code> belongs to using the LDA Bag of Words model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "adf8914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Title  index\n",
      "0    when is it too late to buy the hottest gifts o...      0\n",
      "1              robots vs. fatbergs in america's sewers      1\n",
      "2    bitcoin's creator could be unmasked at florida...      2\n",
      "3     toxic positivity is very real, and very annoying      3\n",
      "4    daylight-saving time ends sunday. four ways to...      4\n",
      "..                                                 ...    ...\n",
      "452          vienna reels from a rare terrorist attack    452\n",
      "453          coronavirus briefing: what happened today    453\n",
      "454  she was losing fistfuls of hair. what was caus...    454\n",
      "455  think you have ‘normal’ blood pressure? think ...    455\n",
      "456  navigating my son’s a.d.h.d. made me realize i...    456\n",
      "\n",
      "[457 rows x 2 columns]\n",
      "\n",
      "Score: 0.10000000149011612\n",
      "Topic: 0.130*\"make\" + 0.096*\"american\" + 0.060*\"tri\" + 0.060*\"game\" + 0.049*\"best\" + 0.049*\"old\" + 0.043*\"power\" + 0.037*\"rule\" + 0.037*\"year\" + 0.037*\"electr\"\n",
      "\n",
      "Score: 0.10000000149011612\n",
      "Topic: 0.117*\"cup\" + 0.112*\"team\" + 0.100*\"lead\" + 0.090*\"ryder\" + 0.056*\"final\" + 0.041*\"week\" + 0.031*\"busi\" + 0.031*\"best\" + 0.027*\"world\" + 0.026*\"win\"\n",
      "\n",
      "Score: 0.10000000149011612\n",
      "Topic: 0.135*\"covid\" + 0.118*\"democrat\" + 0.097*\"climat\" + 0.070*\"plan\" + 0.057*\"virus\" + 0.043*\"time\" + 0.043*\"report\" + 0.036*\"world\" + 0.022*\"take\" + 0.022*\"work\"\n",
      "\n",
      "Score: 0.10000000149011612\n",
      "Topic: 0.152*\"new\" + 0.131*\"review\" + 0.071*\"long\" + 0.068*\"big\" + 0.067*\"appl\" + 0.066*\"season\" + 0.054*\"electr\" + 0.049*\"vaccin\" + 0.041*\"say\" + 0.023*\"week\"\n",
      "\n",
      "Score: 0.10000000149011612\n",
      "Topic: 0.083*\"war\" + 0.078*\"chang\" + 0.072*\"kill\" + 0.072*\"star\" + 0.066*\"space\" + 0.065*\"covid\" + 0.058*\"rule\" + 0.058*\"colleg\" + 0.049*\"report\" + 0.031*\"offer\"\n",
      "\n",
      "Score: 0.10000000149011612\n",
      "Topic: 0.163*\"new\" + 0.068*\"inflat\" + 0.059*\"fight\" + 0.050*\"retir\" + 0.050*\"onlin\" + 0.046*\"biden\" + 0.046*\"life\" + 0.041*\"american\" + 0.037*\"work\" + 0.037*\"insid\"\n",
      "\n",
      "Score: 0.10000000149011612\n",
      "Topic: 0.186*\"home\" + 0.095*\"world\" + 0.079*\"lose\" + 0.073*\"nasa\" + 0.073*\"face\" + 0.067*\"arrest\" + 0.037*\"space\" + 0.025*\"die\" + 0.019*\"best\" + 0.019*\"plan\"\n",
      "\n",
      "Score: 0.10000000149011612\n",
      "Topic: 0.150*\"climat\" + 0.095*\"talk\" + 0.084*\"money\" + 0.078*\"high\" + 0.073*\"glasgow\" + 0.057*\"beat\" + 0.046*\"year\" + 0.046*\"return\" + 0.035*\"old\" + 0.029*\"chang\"\n",
      "\n",
      "Score: 0.10000000149011612\n",
      "Topic: 0.092*\"start\" + 0.086*\"charg\" + 0.079*\"new\" + 0.072*\"time\" + 0.072*\"face\" + 0.072*\"golf\" + 0.072*\"basketbal\" + 0.044*\"health\" + 0.040*\"colleg\" + 0.037*\"busi\"\n",
      "\n",
      "Score: 0.10000000149011612\n",
      "Topic: 0.096*\"brief\" + 0.088*\"playoff\" + 0.084*\"die\" + 0.080*\"worker\" + 0.072*\"coronavirus\" + 0.072*\"colleg\" + 0.063*\"footbal\" + 0.055*\"america\" + 0.047*\"art\" + 0.043*\"elect\"\n"
     ]
    }
   ],
   "source": [
    "print(documents)  # just to remember what it was about\n",
    "for index, score in sorted(lda_model[bow_corpus[docno]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\nTopic: {}\".format(score,lda_model.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c36ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afd882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
